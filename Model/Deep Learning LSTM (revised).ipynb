{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02170c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.14.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.30)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.25.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.14.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.73.1)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.4.30)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.40.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.32.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy scikit-learn matplotlib tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61e73d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4573d169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras-tuner) (2.12.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras-tuner) (25.0)\n",
      "Requirement already satisfied: requests in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras-tuner) (2.32.4)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->keras-tuner) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->keras-tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->keras-tuner) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->keras-tuner) (2025.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b84d7c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertopic[all] in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (0.17.0)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from bertopic[all]) (0.8.40)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from bertopic[all]) (1.23.5)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from bertopic[all]) (2.1.4)\n",
      "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from bertopic[all]) (6.0.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from bertopic[all]) (1.7.0)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from bertopic[all]) (4.1.0)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from bertopic[all]) (4.67.1)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from bertopic[all]) (0.5.7)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from hdbscan>=0.8.29->bertopic[all]) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from hdbscan>=0.8.29->bertopic[all]) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas>=1.1.5->bertopic[all]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas>=1.1.5->bertopic[all]) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas>=1.1.5->bertopic[all]) (2025.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from plotly>=4.7.0->bertopic[all]) (1.38.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from plotly>=4.7.0->bertopic[all]) (25.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn>=1.0->bertopic[all]) (3.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (4.51.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (2.7.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (0.31.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (9.5.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (4.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tqdm>=4.41.1->bertopic[all]) (0.4.6)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from umap-learn>=0.5.0->bertopic[all]) (0.61.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from umap-learn>=0.5.0->bertopic[all]) (0.5.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[all]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[all]) (2025.3.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[all]) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[all]) (2.32.4)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic[all]) (0.44.0)\n",
      "Collecting numpy>=1.20.0 (from bertopic[all])\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic[all]) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[all]) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[all]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[all]) (3.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[all]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[all]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[all]) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[all]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[all]) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[all]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[all]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[all]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[all]) (2025.6.15)\n",
      "Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: bertopic 0.17.0 does not provide the extra 'all'\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\n",
      "streamlit 1.45.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "tensorflow-intel 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install bertopic[all]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c06e50e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: tf-keras in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (2.19.0)\n",
      "Collecting tensorflow<2.20,>=2.19 (from tf-keras)\n",
      "  Using cached tensorflow-2.19.0-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.25.8)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.14.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.73.1)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.6.15)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Using cached tensorflow-2.19.0-cp310-cp310-win_amd64.whl (375.7 MB)\n",
      "Downloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.5/1.4 MB 762.0 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 907.1 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.0/1.4 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.0/1.4 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.0/1.4 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.0/1.4 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.0/1.4 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.3/1.4 MB 528.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 522.2 kB/s eta 0:00:00\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Installing collected packages: tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.12.3\n",
      "    Uninstalling tensorboard-2.12.3:\n",
      "      Successfully uninstalled tensorboard-2.12.3\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.12.0\n",
      "    Uninstalling keras-2.12.0:\n",
      "      Successfully uninstalled keras-2.12.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.12.0\n",
      "    Uninstalling tensorflow-2.12.0:\n",
      "      Successfully uninstalled tensorflow-2.12.0\n",
      "Successfully installed keras-3.10.0 tensorboard-2.19.0 tensorflow-2.19.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.12.0 requires keras<2.13,>=2.12.0, but you have keras 3.10.0 which is incompatible.\n",
      "tensorflow-intel 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow-intel 2.12.0 requires tensorboard<2.13,>=2.12, but you have tensorboard 2.19.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc0a194f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: torch in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (0.31.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (9.5.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm sentence-transformers torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d972deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (0.14.4)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from statsmodels) (1.26.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from statsmodels) (1.15.3)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from statsmodels) (2.1.4)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install statsmodels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d32b5",
   "metadata": {},
   "source": [
    "LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55265718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with walk-forward validation, tqdm progress bar, ETA, and baseline comparison\n",
    "#KALAU MAU PAKAI GPU\n",
    "#import tensorflow as tf\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "#if gpus:\n",
    "#    print(\"‚úÖ GPU tersedia:\", gpus)\n",
    "#    try:\n",
    "#        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "#    except RuntimeError as e:\n",
    "#        print(e)\n",
    "#else:\n",
    "#    print(\"‚ö†Ô∏è GPU tidak tersedia, menggunakan CPU\")\n",
    "\n",
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"2\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, concatenate, TimeDistributed, BatchNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PRICE_FEATURES = ['open', 'high', 'low', 'close', 'volume']\n",
    "aapl = pd.read_csv(\"AAPL_2020_2025.csv\")\n",
    "apple_df = pd.read_csv(\"apple_with_sentiment.csv\")\n",
    "\n",
    "for df in [aapl, apple_df]:\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def run_topic_pipeline(news_df):\n",
    "    embeddings = embedding_model.encode(news_df['content'].tolist(), show_progress_bar=True)\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        calculate_probabilities=True,\n",
    "        min_topic_size=15,\n",
    "        low_memory=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(news_df['content'], embeddings)\n",
    "    news_df['topic_prob'] = [np.array(p) for p in probs]\n",
    "\n",
    "    num_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
    "    daily_topic_sent = {}\n",
    "    for date, group in news_df.groupby('date'):\n",
    "        topic_sent = np.zeros(num_topics)\n",
    "        topic_weight = np.zeros(num_topics)\n",
    "        for _, row in group.iterrows():\n",
    "            for i in range(num_topics):\n",
    "                topic_sent[i] += row['sentiment_score'] * row['topic_prob'][i]\n",
    "                topic_weight[i] += row['topic_prob'][i]\n",
    "        avg_sent = [topic_sent[i]/topic_weight[i] if topic_weight[i]!=0 else 0 for i in range(num_topics)]\n",
    "        daily_topic_sent[date] = avg_sent\n",
    "\n",
    "    sent_df = pd.DataFrame.from_dict(daily_topic_sent, orient='index')\n",
    "    sent_df.index = pd.to_datetime(sent_df.index).tz_localize(None).normalize()\n",
    "    return sent_df\n",
    "\n",
    "topic_aapl = run_topic_pipeline(apple_df)\n",
    "\n",
    "def merge_price_topic(stock_df, topic_df):\n",
    "    df = stock_df.copy()\n",
    "    df = df[df['date'] >= '2021-01-01']\n",
    "    df.set_index('date', inplace=True)\n",
    "    return pd.merge(df, topic_df, left_index=True, right_index=True, how='inner').dropna()\n",
    "\n",
    "merged_aapl = merge_price_topic(aapl, topic_aapl)\n",
    "WINDOW_SIZE = 360\n",
    "\n",
    "def make_dataset(df, window_size):\n",
    "    X_price, X_topic, y, dates = [], [], [], []\n",
    "    scaler = StandardScaler()\n",
    "    scaled_prices = scaler.fit_transform(df[PRICE_FEATURES])\n",
    "    for i in range(window_size, len(df) - 1):\n",
    "        price = scaled_prices[i-window_size:i]\n",
    "        topic = df.iloc[i-window_size:i, len(PRICE_FEATURES):].to_numpy(dtype=np.float32)\n",
    "        target = df.iloc[i+1]['close']\n",
    "        date = df.index[i+1]\n",
    "        X_price.append(price)\n",
    "        X_topic.append(topic)\n",
    "        y.append(target)\n",
    "        dates.append(date)\n",
    "    return (\n",
    "        np.array(X_price, dtype=np.float32),\n",
    "        np.array(X_topic, dtype=np.float32),\n",
    "        np.array(y, dtype=np.float32),\n",
    "        dates\n",
    "    )\n",
    "\n",
    "Xp, Xt, y, y_dates = make_dataset(merged_aapl, WINDOW_SIZE)\n",
    "\n",
    "def build_lstm_model(Xp, Xt):\n",
    "    in_price = Input(shape=(Xp.shape[1], Xp.shape[2]))\n",
    "    x1 = LSTM(64, return_sequences=True)(in_price)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    x1 = LSTM(16)(x1)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "\n",
    "    in_topic = Input(shape=(Xt.shape[1], Xt.shape[2]))\n",
    "    x2 = TimeDistributed(Dense(64, activation='relu'))(in_topic)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = TimeDistributed(Dense(32, activation='relu'))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = GlobalAveragePooling1D()(x2)\n",
    "\n",
    "    x = concatenate([x1, x2])\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=[in_price, in_topic], outputs=out)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')\n",
    "    return model\n",
    "\n",
    "def walk_forward_lstm(Xp, Xt, y, dates, start=0.8):\n",
    "    n_total = len(y)\n",
    "    n_train = int(n_total * start)\n",
    "    y_pred, y_true = [], []\n",
    "    start_all = time.time()\n",
    "    for i in tqdm(range(n_train, n_total), desc=\"Walk-Forward LSTM\"):\n",
    "        t0 = time.time()\n",
    "        model = build_lstm_model(Xp, Xt)\n",
    "        model.fit([Xp[:i], Xt[:i]], y[:i], epochs=15, batch_size=32, verbose=0)\n",
    "        pred = model.predict([Xp[i:i+1], Xt[i:i+1]], verbose=0)[0][0]\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(y[i])\n",
    "        t1 = time.time()\n",
    "        print(f\"Step {i}/{n_total} - Time: {t1 - t0:.2f}s\")\n",
    "    print(f\"Total time: {(time.time() - start_all)/60:.2f} minutes\")\n",
    "    return y_true, y_pred\n",
    "\n",
    "def evaluate_baseline(y_true, y_pred, label=\"Model\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"üìä {label} ‚Äî MAE: {mae:.2f}, RMSE: {rmse:.2f}, R¬≤: {r2:.4f}\")\n",
    "    return mae, rmse, r2\n",
    "\n",
    "def baseline_random_walk(y_true):\n",
    "    return y_true[:-1], y_true[1:]\n",
    "\n",
    "def baseline_arima(df_close):\n",
    "    history = list(df_close[:WINDOW_SIZE])\n",
    "    predictions = []\n",
    "    for t in range(WINDOW_SIZE, len(df_close)-1):\n",
    "        model = ARIMA(history, order=(5,1,0))\n",
    "        model_fit = model.fit()\n",
    "        output = model_fit.forecast()\n",
    "        predictions.append(output[0])\n",
    "        history.append(df_close[t])\n",
    "    return df_close[WINDOW_SIZE+1:], predictions[1:]\n",
    "\n",
    "def baseline_sarima(df_close):\n",
    "    history = list(df_close[:WINDOW_SIZE])\n",
    "    predictions = []\n",
    "    for t in range(WINDOW_SIZE, len(df_close)-1):\n",
    "        model = SARIMAX(history, order=(1,1,1), seasonal_order=(1,1,1,12))\n",
    "        model_fit = model.fit(disp=False)\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(df_close[t])\n",
    "    return df_close[WINDOW_SIZE+1:], predictions[1:]\n",
    "\n",
    "print(\"\\nüîÅ Training LSTM with Walk-Forward Validation + ETA...\")\n",
    "y_true_lstm, y_pred_lstm = walk_forward_lstm(Xp, Xt, y, y_dates)\n",
    "evaluate_baseline(y_true_lstm, y_pred_lstm, \"LSTM\")\n",
    "\n",
    "print(\"\\nüìâ Evaluating Baseline Models...\")\n",
    "y_rw_true, y_rw_pred = baseline_random_walk(list(y))\n",
    "evaluate_baseline(y_rw_true, y_rw_pred, \"Random Walk\")\n",
    "\n",
    "y_arima_true, y_arima_pred = baseline_arima(list(merged_aapl['close'].values))\n",
    "evaluate_baseline(y_arima_true, y_arima_pred, \"ARIMA\")\n",
    "\n",
    "y_sarima_true, y_sarima_pred = baseline_sarima(list(merged_aapl['close'].values))\n",
    "evaluate_baseline(y_sarima_true, y_sarima_pred, \"SARIMA\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_true_lstm[:100], label=\"Actual\", marker='o')\n",
    "plt.plot(y_pred_lstm[:100], label=\"LSTM Predicted\", marker='x')\n",
    "plt.title(\"AAPL - LSTM Walk-Forward Prediction (First 100 Days)\")\n",
    "plt.legend(); plt.grid(); plt.show()\n",
    "\n",
    "\n",
    "# üíæ Simpan prediksi untuk uji statistik\n",
    "np.save(\"y_pred_lstm.npy\", np.array(y_pred_lstm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43728bc0",
   "metadata": {},
   "source": [
    "Versi coba tanpa Retracing / Loopnya di luar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with optimized walk-forward (no retracing on predict)\n",
    "#KALAU MAU PAKAI GPU\n",
    "#import tensorflow as tf\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "#if gpus:\n",
    "#    print(\"‚úÖ GPU tersedia:\", gpus)\n",
    "#    try:\n",
    "#        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "#    except RuntimeError as e:\n",
    "#        print(e)\n",
    "#else:\n",
    "#    print(\"‚ö†Ô∏è GPU tidak tersedia, menggunakan CPU\")\n",
    "\n",
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, concatenate, TimeDistributed, BatchNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PRICE_FEATURES = ['open', 'high', 'low', 'close', 'volume']\n",
    "aapl = pd.read_csv(\"AAPL_2020_2025.csv\")\n",
    "apple_df = pd.read_csv(\"apple_with_sentiment.csv\")\n",
    "\n",
    "for df in [aapl, apple_df]:\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def run_topic_pipeline(news_df):\n",
    "    embeddings = embedding_model.encode(news_df['content'].tolist(), show_progress_bar=True)\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        calculate_probabilities=True,\n",
    "        min_topic_size=15,\n",
    "        low_memory=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(news_df['content'], embeddings)\n",
    "    news_df['topic_prob'] = [np.array(p) for p in probs]\n",
    "\n",
    "    num_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
    "    daily_topic_sent = {}\n",
    "    for date, group in news_df.groupby('date'):\n",
    "        topic_sent = np.zeros(num_topics)\n",
    "        topic_weight = np.zeros(num_topics)\n",
    "        for _, row in group.iterrows():\n",
    "            for i in range(num_topics):\n",
    "                topic_sent[i] += row['sentiment_score'] * row['topic_prob'][i]\n",
    "                topic_weight[i] += row['topic_prob'][i]\n",
    "        avg_sent = [topic_sent[i]/topic_weight[i] if topic_weight[i]!=0 else 0 for i in range(num_topics)]\n",
    "        daily_topic_sent[date] = avg_sent\n",
    "\n",
    "    sent_df = pd.DataFrame.from_dict(daily_topic_sent, orient='index')\n",
    "    sent_df.index = pd.to_datetime(sent_df.index).tz_localize(None).normalize()\n",
    "    return sent_df\n",
    "\n",
    "topic_aapl = run_topic_pipeline(apple_df)\n",
    "\n",
    "def merge_price_topic(stock_df, topic_df):\n",
    "    df = stock_df.copy()\n",
    "    df = df[df['date'] >= '2021-01-01']\n",
    "    df.set_index('date', inplace=True)\n",
    "    return pd.merge(df, topic_df, left_index=True, right_index=True, how='inner').dropna()\n",
    "\n",
    "merged_aapl = merge_price_topic(aapl, topic_aapl)\n",
    "WINDOW_SIZE = 360\n",
    "\n",
    "def make_dataset(df, window_size):\n",
    "    X_price, X_topic, y, dates = [], [], [], []\n",
    "    scaler = StandardScaler()\n",
    "    scaled_prices = scaler.fit_transform(df[PRICE_FEATURES])\n",
    "    for i in range(window_size, len(df) - 1):\n",
    "        price = scaled_prices[i-window_size:i]\n",
    "        topic = df.iloc[i-window_size:i, len(PRICE_FEATURES):].to_numpy(dtype=np.float32)\n",
    "        target = df.iloc[i+1]['close']\n",
    "        date = df.index[i+1]\n",
    "        X_price.append(price)\n",
    "        X_topic.append(topic)\n",
    "        y.append(target)\n",
    "        dates.append(date)\n",
    "    return (\n",
    "        np.array(X_price, dtype=np.float32),\n",
    "        np.array(X_topic, dtype=np.float32),\n",
    "        np.array(y, dtype=np.float32),\n",
    "        dates\n",
    "    )\n",
    "\n",
    "Xp, Xt, y, y_dates = make_dataset(merged_aapl, WINDOW_SIZE)\n",
    "\n",
    "\n",
    "def build_lstm_model(Xp, Xt):\n",
    "    in_price = Input(shape=(Xp.shape[1], Xp.shape[2]))\n",
    "    x1 = LSTM(64, return_sequences=True)(in_price)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    x1 = LSTM(16)(x1)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "\n",
    "    in_topic = Input(shape=(Xt.shape[1], Xt.shape[2]))\n",
    "    x2 = TimeDistributed(Dense(64, activation='relu'))(in_topic)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = TimeDistributed(Dense(32, activation='relu'))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = GlobalAveragePooling1D()(x2)\n",
    "\n",
    "    x = concatenate([x1, x2])\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=[in_price, in_topic], outputs=out)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')\n",
    "    return model\n",
    "\n",
    "def walk_forward_lstm_no_retrace(Xp, Xt, y, dates, start=0.8, epochs=10):\n",
    "    n_total = len(y)\n",
    "    n_train = int(n_total * start)\n",
    "    y_pred, y_true = [], []\n",
    "    start_all = time.time()\n",
    "    model = build_lstm_model(Xp, Xt)  # model hanya dibuat 1x\n",
    "    for i in tqdm(range(n_train, n_total), desc=\"Walk-Forward LSTM (No Retrace)\"):\n",
    "        t0 = time.time()\n",
    "        model.fit([Xp[:i], Xt[:i]], y[:i], epochs=epochs, batch_size=32, verbose=0)\n",
    "        pred_fn = model.predict  # ambil pointer ke fungsi predict\n",
    "        pred = pred_fn([Xp[i:i+1], Xt[i:i+1]], verbose=0)[0][0]\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(y[i])\n",
    "        print(f\"Step {i}/{n_total} - Time: {time.time() - t0:.2f}s\")\n",
    "    print(f\"Total time: {(time.time() - start_all)/60:.2f} minutes\")\n",
    "    return y_true, y_pred\n",
    "\n",
    "def evaluate_baseline(y_true, y_pred, label=\"Model\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"üìä {label} ‚Äî MAE: {mae:.2f}, RMSE: {rmse:.2f}, R¬≤: {r2:.4f}\")\n",
    "    return mae, rmse, r2\n",
    "\n",
    "def baseline_random_walk(y_true):\n",
    "    return y_true[:-1], y_true[1:]\n",
    "\n",
    "def baseline_arima(df_close):\n",
    "    history = list(df_close[:WINDOW_SIZE])\n",
    "    predictions = []\n",
    "    for t in range(WINDOW_SIZE, len(df_close)-1):\n",
    "        model = ARIMA(history, order=(5,1,0))\n",
    "        model_fit = model.fit()\n",
    "        output = model_fit.forecast()\n",
    "        predictions.append(output[0])\n",
    "        history.append(df_close[t])\n",
    "    return df_close[WINDOW_SIZE+1:], predictions[1:]\n",
    "\n",
    "def baseline_sarima(df_close):\n",
    "    history = list(df_close[:WINDOW_SIZE])\n",
    "    predictions = []\n",
    "    for t in range(WINDOW_SIZE, len(df_close)-1):\n",
    "        model = SARIMAX(history, order=(1,1,1), seasonal_order=(1,1,1,12))\n",
    "        model_fit = model.fit(disp=False)\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(df_close[t])\n",
    "    return df_close[WINDOW_SIZE+1:], predictions[1:]\n",
    "\n",
    "print(\"\\nüîÅ Training LSTM with Walk-Forward Validation + ETA...\")\n",
    "y_true_lstm, y_pred_lstm = walk_forward_lstm_no_retrace(Xp, Xt, y, y_dates)\n",
    "evaluate_baseline(y_true_lstm, y_pred_lstm, \"LSTM\")\n",
    "\n",
    "print(\"\\nüìâ Evaluating Baseline Models...\")\n",
    "y_rw_true, y_rw_pred = baseline_random_walk(list(y))\n",
    "evaluate_baseline(y_rw_true, y_rw_pred, \"Random Walk\")\n",
    "\n",
    "y_arima_true, y_arima_pred = baseline_arima(list(merged_aapl['close'].values))\n",
    "evaluate_baseline(y_arima_true, y_arima_pred, \"ARIMA\")\n",
    "\n",
    "y_sarima_true, y_sarima_pred = baseline_sarima(list(merged_aapl['close'].values))\n",
    "evaluate_baseline(y_sarima_true, y_sarima_pred, \"SARIMA\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_true_lstm[:100], label=\"Actual\", marker='o')\n",
    "plt.plot(y_pred_lstm[:100], label=\"LSTM Predicted\", marker='x')\n",
    "plt.title(\"AAPL - LSTM Walk-Forward Prediction (First 100 Days)\")\n",
    "plt.legend(); plt.grid(); plt.show()\n",
    "\n",
    "\n",
    "# üíæ Simpan prediksi untuk uji statistik\n",
    "np.save(\"y_pred_lstm.npy\", np.array(y_pred_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed102832",
   "metadata": {},
   "source": [
    "Versi Pakai Expanding Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b28e78e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error, r2_score\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\__init__.py:73\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     __check_build,\n\u001b[0;32m     71\u001b[0m     _distributor_init,\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     76\u001b[0m _submodules \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    115\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\__init__.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     resample,\n\u001b[0;32m     19\u001b[0m     shuffle,\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\scipy\\sparse\\__init__.py:315\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_index_dtype, safely_cast_index_arrays\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# For backward compatibility with v0.19.\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csgraph\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# Deprecated namespaces, to be removed in v2.0.0\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    319\u001b[0m     base, bsr, compressed, construct, coo, csc, csr, data, dia, dok, extract,\n\u001b[0;32m    320\u001b[0m     lil, sparsetools, sputils\n\u001b[0;32m    321\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\scipy\\sparse\\csgraph\\__init__.py:187\u001b[0m\n\u001b[0;32m    158\u001b[0m __docformat__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestructuredtext en\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconnected_components\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    161\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlaplacian\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    162\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshortest_path\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsgraph_to_masked\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    185\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNegativeCycleError\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_laplacian\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m laplacian\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shortest_path\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    189\u001b[0m     shortest_path, floyd_warshall, dijkstra, bellman_ford, johnson, yen,\n\u001b[0;32m    190\u001b[0m     NegativeCycleError\n\u001b[0;32m    191\u001b[0m )\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traversal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    193\u001b[0m     breadth_first_order, depth_first_order, breadth_first_tree,\n\u001b[0;32m    194\u001b[0m     depth_first_tree, connected_components\n\u001b[0;32m    195\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\scipy\\sparse\\csgraph\\_laplacian.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearOperator\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_pydata_sparse_to_scipy, is_pydata_spmatrix\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m###############################################################################\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Graph laplacian\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\scipy\\sparse\\linalg\\__init__.py:131\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mSparse linear algebra (:mod:`scipy.sparse.linalg`)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m==================================================\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m \n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_isolve\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dsolve\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_interface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\scipy\\sparse\\linalg\\_isolve\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterative Solvers for Sparse Linear Systems\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#from info import __doc__\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterative\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mminres\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m minres\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlgmres\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lgmres\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\scipy\\sparse\\linalg\\_isolve\\iterative.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_interface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearOperator\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_system\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbicg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbicgstab\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcgs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgmres\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqmr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_atol_rtol\u001b[39m(name, b_norm, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\scipy\\linalg\\__init__.py:203\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m====================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mLinear algebra (:mod:`scipy.linalg`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_misc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cythonized_array_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_basic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\scipy\\linalg\\_misc.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinAlgError\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinAlgError\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinAlgWarning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\scipy\\linalg\\blas.py:213\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _fblas\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _cblas\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LSTM with expanding-window validation (step=5) + baselines + metrics + plot\n",
    "#KALAU MAU PAKAI GPU\n",
    "#import tensorflow as tf\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "#if gpus:\n",
    "#    print(\"‚úÖ GPU tersedia:\", gpus)\n",
    "#    try:\n",
    "#        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "#    except RuntimeError as e:\n",
    "#        print(e)\n",
    "#else:\n",
    "#    print(\"‚ö†Ô∏è GPU tidak tersedia, menggunakan CPU\")\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, concatenate, TimeDistributed, BatchNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PRICE_FEATURES = ['open', 'high', 'low', 'close', 'volume']\n",
    "aapl = pd.read_csv(\"AAPL_2020_2025.csv\")\n",
    "apple_df = pd.read_csv(\"apple_with_sentiment.csv\")\n",
    "\n",
    "for df in [aapl, apple_df]:\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def run_topic_pipeline(news_df):\n",
    "    embeddings = embedding_model.encode(news_df['content'].tolist(), show_progress_bar=True)\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        calculate_probabilities=True,\n",
    "        min_topic_size=15,\n",
    "        low_memory=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(news_df['content'], embeddings)\n",
    "    news_df['topic_prob'] = [np.array(p) for p in probs]\n",
    "\n",
    "    num_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
    "    daily_topic_sent = {}\n",
    "    for date, group in news_df.groupby('date'):\n",
    "        topic_sent = np.zeros(num_topics)\n",
    "        topic_weight = np.zeros(num_topics)\n",
    "        for _, row in group.iterrows():\n",
    "            for i in range(num_topics):\n",
    "                topic_sent[i] += row['sentiment_score'] * row['topic_prob'][i]\n",
    "                topic_weight[i] += row['topic_prob'][i]\n",
    "        avg_sent = [topic_sent[i]/topic_weight[i] if topic_weight[i]!=0 else 0 for i in range(num_topics)]\n",
    "        daily_topic_sent[date] = avg_sent\n",
    "\n",
    "    sent_df = pd.DataFrame.from_dict(daily_topic_sent, orient='index')\n",
    "    sent_df.index = pd.to_datetime(sent_df.index).tz_localize(None).normalize()\n",
    "    return sent_df\n",
    "\n",
    "topic_aapl = run_topic_pipeline(apple_df)\n",
    "\n",
    "def merge_price_topic(stock_df, topic_df):\n",
    "    df = stock_df.copy()\n",
    "    df = df[df['date'] >= '2021-01-01']\n",
    "    df.set_index('date', inplace=True)\n",
    "    return pd.merge(df, topic_df, left_index=True, right_index=True, how='inner').dropna()\n",
    "\n",
    "merged_aapl = merge_price_topic(aapl, topic_aapl)\n",
    "WINDOW_SIZE = 360\n",
    "\n",
    "def make_dataset(df, window_size):\n",
    "    X_price, X_topic, y, dates = [], [], [], []\n",
    "    scaler = StandardScaler()\n",
    "    scaled_prices = scaler.fit_transform(df[PRICE_FEATURES])\n",
    "    for i in range(window_size, len(df) - 1):\n",
    "        price = scaled_prices[i-window_size:i]\n",
    "        topic = df.iloc[i-window_size:i, len(PRICE_FEATURES):].to_numpy(dtype=np.float32)\n",
    "        target = df.iloc[i+1]['close']\n",
    "        date = df.index[i+1]\n",
    "        X_price.append(price)\n",
    "        X_topic.append(topic)\n",
    "        y.append(target)\n",
    "        dates.append(date)\n",
    "    return (\n",
    "        np.array(X_price, dtype=np.float32),\n",
    "        np.array(X_topic, dtype=np.float32),\n",
    "        np.array(y, dtype=np.float32),\n",
    "        dates\n",
    "    )\n",
    "\n",
    "Xp, Xt, y, y_dates = make_dataset(merged_aapl, WINDOW_SIZE)\n",
    "\n",
    "def build_lstm_model(Xp, Xt):\n",
    "    in_price = Input(shape=(Xp.shape[1], Xp.shape[2]))\n",
    "    x1 = LSTM(64, return_sequences=True)(in_price)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    x1 = LSTM(16)(x1)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "\n",
    "    in_topic = Input(shape=(Xt.shape[1], Xt.shape[2]))\n",
    "    x2 = TimeDistributed(Dense(64, activation='relu'))(in_topic)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = TimeDistributed(Dense(32, activation='relu'))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = GlobalAveragePooling1D()(x2)\n",
    "\n",
    "    x = concatenate([x1, x2])\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=[in_price, in_topic], outputs=out)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')\n",
    "    return model\n",
    "\n",
    "def walk_forward_lstm_expanding_step5(Xp, Xt, y, dates, start=0.8, epochs=10):\n",
    "    n_total = len(y)\n",
    "    n_train = int(n_total * start)\n",
    "    y_pred, y_true = [], []\n",
    "    model = build_lstm_model(Xp, Xt)\n",
    "\n",
    "    step_durations = []\n",
    "\n",
    "    for i in tqdm(range(n_train, n_total, 5), desc=\"Expanding Step=5\"):\n",
    "        start_time = time.time()  # ‚è±Ô∏è Start timer\n",
    "\n",
    "        model.fit([Xp[:i], Xt[:i]], y[:i], epochs=epochs, batch_size=32, verbose=0)\n",
    "        for j in range(i, min(i+5, n_total)):\n",
    "            pred = model.predict([Xp[j:j+1], Xt[j:j+1]], verbose=0)[0][0]\n",
    "            y_pred.append(pred)\n",
    "            y_true.append(y[j])\n",
    "        \n",
    "        step_time = time.time() - start_time\n",
    "        step_durations.append(step_time)\n",
    "        avg_time = np.mean(step_durations)\n",
    "        remaining_steps = (n_total - i - 1) // 5\n",
    "        eta_minutes = (remaining_steps * avg_time) / 60\n",
    "\n",
    "        print(f\"Step {i}‚Äì{min(i+5, n_total)-1} done in {step_time:.2f} sec | ETA: {eta_minutes:.2f} min remaining\")\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "def evaluate_baseline(y_true, y_pred, label=\"Model\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"üìä {label} ‚Äî MAE: {mae:.2f}, RMSE: {rmse:.2f}, R¬≤: {r2:.4f}\")\n",
    "    return mae, rmse, r2\n",
    "\n",
    "def baseline_random_walk(y_true):\n",
    "    return y_true[:-1], y_true[1:]\n",
    "\n",
    "def baseline_arima(df_close):\n",
    "    history = list(df_close[:WINDOW_SIZE])\n",
    "    predictions = []\n",
    "    for t in range(WINDOW_SIZE, len(df_close)-1):\n",
    "        model = ARIMA(history, order=(5,1,0))\n",
    "        model_fit = model.fit()\n",
    "        output = model_fit.forecast()\n",
    "        predictions.append(output[0])\n",
    "        history.append(df_close[t])\n",
    "    return df_close[WINDOW_SIZE+1:], predictions[1:]\n",
    "\n",
    "def baseline_sarima(df_close):\n",
    "    history = list(df_close[:WINDOW_SIZE])\n",
    "    predictions = []\n",
    "    for t in range(WINDOW_SIZE, len(df_close)-1):\n",
    "        model = SARIMAX(history, order=(1,1,1), seasonal_order=(1,1,1,12))\n",
    "        model_fit = model.fit(disp=False)\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(df_close[t])\n",
    "    return df_close[WINDOW_SIZE+1:], predictions[1:]\n",
    "\n",
    "print(\"\\nüîÅ Training LSTM with Expanding Step=5...\")\n",
    "y_true_lstm, y_pred_lstm = walk_forward_lstm_expanding_step5(Xp, Xt, y, y_dates)\n",
    "\n",
    "# Simpan prediksi dan ground-truth untuk uji statistik\n",
    "np.save(\"y_true_lstm_step5.npy\", np.array(y_true_lstm))\n",
    "np.save(\"y_pred_lstm_step5.npy\", np.array(y_pred_lstm))\n",
    "\n",
    "# Evaluasi LSTM\n",
    "evaluate_baseline(y_true_lstm, y_pred_lstm, \"LSTM (Step=5)\")\n",
    "\n",
    "# Evaluasi Baselines\n",
    "print(\"\\nüìâ Evaluating Baseline Models...\")\n",
    "y_rw_true, y_rw_pred = baseline_random_walk(list(y))\n",
    "evaluate_baseline(y_rw_true, y_rw_pred, \"Random Walk\")\n",
    "\n",
    "y_arima_true, y_arima_pred = baseline_arima(list(merged_aapl['close'].values))\n",
    "evaluate_baseline(y_arima_true, y_arima_pred, \"ARIMA\")\n",
    "\n",
    "y_sarima_true, y_sarima_pred = baseline_sarima(list(merged_aapl['close'].values))\n",
    "evaluate_baseline(y_sarima_true, y_sarima_pred, \"SARIMA\")\n",
    "\n",
    "# Plot hasil prediksi\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_true_lstm[:100], label=\"Actual\", marker='o')\n",
    "plt.plot(y_pred_lstm[:100], label=\"LSTM Predicted\", marker='x')\n",
    "plt.title(\"AAPL - LSTM Expanding Step=5 Prediction (First 100 Days)\")\n",
    "plt.legend(); plt.grid(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11166124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Cek ketersediaan GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"‚úÖ GPU tersedia:\", gpus)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU tidak tersedia, model akan dilatih di CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6a33df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11003543570168511092\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824f8fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† TensorFlow version: 2.19.0\n",
      "üì¶ Built with CUDA: False\n",
      "üöÄ GPU Available: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"üß† TensorFlow version:\", tf.__version__)\n",
    "print(\"üì¶ Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"üöÄ GPU Available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "497c6429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
