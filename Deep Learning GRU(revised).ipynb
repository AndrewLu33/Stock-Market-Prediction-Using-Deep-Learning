{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02170c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy scikit-learn matplotlib tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61e73d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4573d169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras-tuner) (3.9.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras-tuner) (24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras-tuner) (2.32.3)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras->keras-tuner) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras->keras-tuner) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras->keras-tuner) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras->keras-tuner) (0.0.9)\n",
      "Requirement already satisfied: h5py in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras->keras-tuner) (3.13.0)\n",
      "Requirement already satisfied: optree in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras->keras-tuner) (0.15.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras->keras-tuner) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->keras-tuner) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->keras-tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->keras-tuner) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->keras-tuner) (2025.1.31)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from optree->keras->keras-tuner) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from rich->keras->keras-tuner) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from rich->keras->keras-tuner) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b84d7c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertopic[all] in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (0.17.0)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from bertopic[all]) (0.8.40)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from bertopic[all]) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from bertopic[all]) (2.1.4)\n",
      "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from bertopic[all]) (6.0.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from bertopic[all]) (1.7.0)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from bertopic[all]) (4.1.0)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from bertopic[all]) (4.67.1)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from bertopic[all]) (0.5.7)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from hdbscan>=0.8.29->bertopic[all]) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from hdbscan>=0.8.29->bertopic[all]) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas>=1.1.5->bertopic[all]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas>=1.1.5->bertopic[all]) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas>=1.1.5->bertopic[all]) (2025.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from plotly>=4.7.0->bertopic[all]) (1.38.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from plotly>=4.7.0->bertopic[all]) (24.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn>=1.0->bertopic[all]) (3.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (4.51.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (2.7.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (0.31.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (9.5.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tqdm>=4.41.1->bertopic[all]) (0.4.6)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from umap-learn>=0.5.0->bertopic[all]) (0.61.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from umap-learn>=0.5.0->bertopic[all]) (0.5.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[all]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[all]) (2025.3.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[all]) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[all]) (2.32.3)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic[all]) (0.44.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic[all]) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[all]) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[all]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[all]) (3.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[all]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[all]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[all]) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[all]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[all]) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[all]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[all]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[all]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[all]) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: bertopic 0.17.0 does not provide the extra 'all'\n"
     ]
    }
   ],
   "source": [
    "pip install bertopic[all]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c06e50e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc0a194f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: torch in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (0.31.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (9.5.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm sentence-transformers torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d972deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (0.14.4)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from statsmodels) (1.26.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from statsmodels) (1.11.4)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from statsmodels) (2.1.4)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from statsmodels) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install statsmodels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c81d0c",
   "metadata": {},
   "source": [
    "GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e308878",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error, r2_score\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\__init__.py:73\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     __check_build,\n\u001b[0;32m     71\u001b[0m     _distributor_init,\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     76\u001b[0m _submodules \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    115\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\__init__.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     resample,\n\u001b[0;32m     19\u001b[0m     shuffle,\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\scipy\\sparse\\__init__.py:287\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix_io\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# For backward compatibility with v0.19.\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csgraph\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Deprecated namespaces, to be removed in v2.0.0\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    291\u001b[0m     base, bsr, compressed, construct, coo, csc, csr, data, dia, dok, extract,\n\u001b[0;32m    292\u001b[0m     lil, sparsetools, sputils\n\u001b[0;32m    293\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\scipy\\sparse\\csgraph\\__init__.py:185\u001b[0m\n\u001b[0;32m    157\u001b[0m __docformat__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestructuredtext en\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconnected_components\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    160\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlaplacian\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    161\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshortest_path\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsgraph_to_masked\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    183\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNegativeCycleError\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_laplacian\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m laplacian\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shortest_path\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    187\u001b[0m     shortest_path, floyd_warshall, dijkstra, bellman_ford, johnson,\n\u001b[0;32m    188\u001b[0m     NegativeCycleError\n\u001b[0;32m    189\u001b[0m )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traversal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    191\u001b[0m     breadth_first_order, depth_first_order, breadth_first_tree,\n\u001b[0;32m    192\u001b[0m     depth_first_tree, connected_components\n\u001b[0;32m    193\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\scipy\\sparse\\csgraph\\_laplacian.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearOperator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m###############################################################################\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Graph laplacian\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlaplacian\u001b[39m(\n\u001b[0;32m     13\u001b[0m     csgraph,\n\u001b[0;32m     14\u001b[0m     normed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     symmetrized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     22\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\scipy\\sparse\\linalg\\__init__.py:121\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mSparse linear algebra (:mod:`scipy.sparse.linalg`)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m==================================================\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_isolve\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dsolve\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_interface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_eigen\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\scipy\\sparse\\linalg\\_dsolve\\__init__.py:58\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mLinear Solvers\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m==============\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#import umfpack\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#__doc__ = '\\n\\n'.join( (__doc__,  umfpack.__doc__) )\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m#del umfpack\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinsolve\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_superlu\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SuperLU\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _add_newdocs\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda\\envs\\nlp\\lib\\site-packages\\scipy\\sparse\\linalg\\_dsolve\\linsolve.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinAlgError\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _superlu\n\u001b[0;32m     13\u001b[0m noScikit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#KALAU MAU PAKAI GPU\n",
    "#import tensorflow as tf\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "#if gpus:\n",
    "#    print(\" GPU tersedia:\", gpus)\n",
    "#    try:\n",
    "#        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "#    except RuntimeError as e:\n",
    "#        print(e)\n",
    "#else:\n",
    "#    print(\" GPU tidak tersedia, menggunakan CPU\")\n",
    "\n",
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"2\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Dropout, concatenate, TimeDistributed, BatchNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PRICE_FEATURES = ['open', 'high', 'low', 'close', 'volume']\n",
    "aapl = pd.read_csv(\"AAPL_2020_2025.csv\")\n",
    "apple_df = pd.read_csv(\"apple_with_sentiment.csv\")\n",
    "\n",
    "for df in [aapl, apple_df]:\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def run_topic_pipeline(news_df):\n",
    "    embeddings = embedding_model.encode(news_df['content'].tolist(), show_progress_bar=True)\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        calculate_probabilities=True,\n",
    "        min_topic_size=15,\n",
    "        low_memory=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(news_df['content'], embeddings)\n",
    "    news_df['topic_prob'] = [np.array(p) for p in probs]\n",
    "\n",
    "    num_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
    "    daily_topic_sent = {}\n",
    "    for date, group in news_df.groupby('date'):\n",
    "        topic_sent = np.zeros(num_topics)\n",
    "        topic_weight = np.zeros(num_topics)\n",
    "        for _, row in group.iterrows():\n",
    "            for i in range(num_topics):\n",
    "                topic_sent[i] += row['sentiment_score'] * row['topic_prob'][i]\n",
    "                topic_weight[i] += row['topic_prob'][i]\n",
    "        avg_sent = [topic_sent[i]/topic_weight[i] if topic_weight[i]!=0 else 0 for i in range(num_topics)]\n",
    "        daily_topic_sent[date] = avg_sent\n",
    "\n",
    "    sent_df = pd.DataFrame.from_dict(daily_topic_sent, orient='index')\n",
    "    sent_df.index = pd.to_datetime(sent_df.index).tz_localize(None).normalize()\n",
    "    return sent_df\n",
    "\n",
    "topic_aapl = run_topic_pipeline(apple_df)\n",
    "\n",
    "def merge_price_topic(stock_df, topic_df):\n",
    "    df = stock_df.copy()\n",
    "    df = df[df['date'] >= '2021-01-01']\n",
    "    df.set_index('date', inplace=True)\n",
    "    return pd.merge(df, topic_df, left_index=True, right_index=True, how='inner').dropna()\n",
    "\n",
    "merged_aapl = merge_price_topic(aapl, topic_aapl)\n",
    "WINDOW_SIZE = 360\n",
    "\n",
    "def make_dataset(df, window_size):\n",
    "    X_price, X_topic, y, dates = [], [], [], []\n",
    "    scaler = StandardScaler()\n",
    "    scaled_prices = scaler.fit_transform(df[PRICE_FEATURES])\n",
    "    for i in range(window_size, len(df) - 1):\n",
    "        price = scaled_prices[i-window_size:i]\n",
    "        topic = df.iloc[i-window_size:i, len(PRICE_FEATURES):].to_numpy(dtype=np.float32)\n",
    "        target = df.iloc[i+1]['close']\n",
    "        date = df.index[i+1]\n",
    "        X_price.append(price)\n",
    "        X_topic.append(topic)\n",
    "        y.append(target)\n",
    "        dates.append(date)\n",
    "    return (\n",
    "        np.array(X_price, dtype=np.float32),\n",
    "        np.array(X_topic, dtype=np.float32),\n",
    "        np.array(y, dtype=np.float32),\n",
    "        dates\n",
    "    )\n",
    "\n",
    "Xp, Xt, y, y_dates = make_dataset(merged_aapl, WINDOW_SIZE)\n",
    "\n",
    "def build_gru_model(Xp, Xt):\n",
    "    in_price = Input(shape=(Xp.shape[1], Xp.shape[2]))\n",
    "    x1 = GRU(64, return_sequences=True)(in_price)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    x1 = GRU(16)(x1)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "\n",
    "    in_topic = Input(shape=(Xt.shape[1], Xt.shape[2]))\n",
    "    x2 = TimeDistributed(Dense(64, activation='relu'))(in_topic)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = TimeDistributed(Dense(32, activation='relu'))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = GlobalAveragePooling1D()(x2)\n",
    "\n",
    "    x = concatenate([x1, x2])\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=[in_price, in_topic], outputs=out)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')\n",
    "    return model\n",
    "\n",
    "def walk_forward_gru(Xp, Xt, y, dates, start=0.8):\n",
    "    n_total = len(y)\n",
    "    n_train = int(n_total * start)\n",
    "    y_pred, y_true = [], []\n",
    "    start_all = time.time()\n",
    "    for i in tqdm(range(n_train, n_total), desc=\"Walk-Forward GRU\"):\n",
    "        t0 = time.time()\n",
    "        model = build_gru_model(Xp, Xt)\n",
    "        model.fit([Xp[:i], Xt[:i]], y[:i], epochs=15, batch_size=32, verbose=0)\n",
    "        pred = model.predict([Xp[i:i+1], Xt[i:i+1]], verbose=0)[0][0]\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(y[i])\n",
    "        t1 = time.time()\n",
    "        print(f\"Step {i}/{n_total} - Time: {t1 - t0:.2f}s\")\n",
    "    print(f\"Total time: {(time.time() - start_all)/60:.2f} minutes\")\n",
    "    return y_true, y_pred\n",
    "\n",
    "def evaluate_baseline(y_true, y_pred, label=\"GRU\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\" {label}  MAE: {mae:.2f}, RMSE: {rmse:.2f}, R: {r2:.4f}\")\n",
    "    return mae, rmse, r2\n",
    "\n",
    "def baseline_random_walk(y_true):\n",
    "    return y_true[:-1], y_true[1:]\n",
    "\n",
    "def baseline_arima(df_close):\n",
    "    history = list(df_close[:WINDOW_SIZE])\n",
    "    predictions = []\n",
    "    for t in range(WINDOW_SIZE, len(df_close)-1):\n",
    "        model = ARIMA(history, order=(5,1,0))\n",
    "        model_fit = model.fit()\n",
    "        output = model_fit.forecast()\n",
    "        predictions.append(output[0])\n",
    "        history.append(df_close[t])\n",
    "    return df_close[WINDOW_SIZE+1:], predictions[1:]\n",
    "\n",
    "def baseline_sarima(df_close):\n",
    "    history = list(df_close[:WINDOW_SIZE])\n",
    "    predictions = []\n",
    "    for t in range(WINDOW_SIZE, len(df_close)-1):\n",
    "        model = SARIMAX(history, order=(1,1,1), seasonal_order=(1,1,1,12))\n",
    "        model_fit = model.fit(disp=False)\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(df_close[t])\n",
    "    return df_close[WINDOW_SIZE+1:], predictions[1:]\n",
    "\n",
    "print(\"\\n Training GRU with Walk-Forward Validation + ETA...\")\n",
    "y_true_gru, y_pred_gru = walk_forward_gru(Xp, Xt, y, y_dates)\n",
    "evaluate_baseline(y_true_gru, y_pred_gru, \"GRU\")\n",
    "\n",
    "print(\"\\n Evaluating Baseline Models...\")\n",
    "y_rw_true, y_rw_pred = baseline_random_walk(list(y))\n",
    "evaluate_baseline(y_rw_true, y_rw_pred, \"Random Walk\")\n",
    "\n",
    "y_arima_true, y_arima_pred = baseline_arima(list(merged_aapl['close'].values))\n",
    "evaluate_baseline(y_arima_true, y_arima_pred, \"ARIMA\")\n",
    "\n",
    "y_sarima_true, y_sarima_pred = baseline_sarima(list(merged_aapl['close'].values))\n",
    "evaluate_baseline(y_sarima_true, y_sarima_pred, \"SARIMA\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_true_gru[:100], label=\"Actual\", marker='o')\n",
    "plt.plot(y_pred_gru[:100], label=\"GRU Predicted\", marker='x')\n",
    "plt.title(\"AAPL - GRU Walk-Forward Prediction (First 100 Days)\")\n",
    "plt.legend(); plt.grid(); plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#  Simpan prediksi untuk uji statistik\n",
    "np.save(\"y_true.npy\", np.array(y_true_gru))\n",
    "np.save(\"y_pred_gru.npy\", np.array(y_pred_gru))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48779b1c",
   "metadata": {},
   "source": [
    "Versi coba tanpa Retracing / Loopnya di luar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4d94ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"2\"\n",
    "#KALAU MAU PAKAI GPU\n",
    "#import tensorflow as tf\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "#if gpus:\n",
    "#    print(\" GPU tersedia:\", gpus)\n",
    "#    try:\n",
    "#        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "#    except RuntimeError as e:\n",
    "#        print(e)\n",
    "#else:\n",
    "#    print(\" GPU tidak tersedia, menggunakan CPU\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Dropout, concatenate, TimeDistributed, BatchNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PRICE_FEATURES = ['open', 'high', 'low', 'close', 'volume']\n",
    "aapl = pd.read_csv(\"AAPL_2020_2025.csv\")\n",
    "apple_df = pd.read_csv(\"apple_with_sentiment.csv\")\n",
    "\n",
    "for df in [aapl, apple_df]:\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def run_topic_pipeline(news_df):\n",
    "    embeddings = embedding_model.encode(news_df['content'].tolist(), show_progress_bar=True)\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        calculate_probabilities=True,\n",
    "        min_topic_size=15,\n",
    "        low_memory=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(news_df['content'], embeddings)\n",
    "    news_df['topic_prob'] = [np.array(p) for p in probs]\n",
    "\n",
    "    num_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
    "    daily_topic_sent = {}\n",
    "    for date, group in news_df.groupby('date'):\n",
    "        topic_sent = np.zeros(num_topics)\n",
    "        topic_weight = np.zeros(num_topics)\n",
    "        for _, row in group.iterrows():\n",
    "            for i in range(num_topics):\n",
    "                topic_sent[i] += row['sentiment_score'] * row['topic_prob'][i]\n",
    "                topic_weight[i] += row['topic_prob'][i]\n",
    "        avg_sent = [topic_sent[i]/topic_weight[i] if topic_weight[i]!=0 else 0 for i in range(num_topics)]\n",
    "        daily_topic_sent[date] = avg_sent\n",
    "\n",
    "    sent_df = pd.DataFrame.from_dict(daily_topic_sent, orient='index')\n",
    "    sent_df.index = pd.to_datetime(sent_df.index).tz_localize(None).normalize()\n",
    "    return sent_df\n",
    "\n",
    "topic_aapl = run_topic_pipeline(apple_df)\n",
    "\n",
    "def merge_price_topic(stock_df, topic_df):\n",
    "    df = stock_df.copy()\n",
    "    df = df[df['date'] >= '2021-01-01']\n",
    "    df.set_index('date', inplace=True)\n",
    "    return pd.merge(df, topic_df, left_index=True, right_index=True, how='inner').dropna()\n",
    "\n",
    "merged_aapl = merge_price_topic(aapl, topic_aapl)\n",
    "WINDOW_SIZE = 360\n",
    "\n",
    "def make_dataset(df, window_size):\n",
    "    X_price, X_topic, y, dates = [], [], [], []\n",
    "    scaler = StandardScaler()\n",
    "    scaled_prices = scaler.fit_transform(df[PRICE_FEATURES])\n",
    "    for i in range(window_size, len(df) - 1):\n",
    "        price = scaled_prices[i-window_size:i]\n",
    "        topic = df.iloc[i-window_size:i, len(PRICE_FEATURES):].to_numpy(dtype=np.float32)\n",
    "        target = df.iloc[i+1]['close']\n",
    "        date = df.index[i+1]\n",
    "        X_price.append(price)\n",
    "        X_topic.append(topic)\n",
    "        y.append(target)\n",
    "        dates.append(date)\n",
    "    return (\n",
    "        np.array(X_price, dtype=np.float32),\n",
    "        np.array(X_topic, dtype=np.float32),\n",
    "        np.array(y, dtype=np.float32),\n",
    "        dates\n",
    "    )\n",
    "\n",
    "Xp, Xt, y, y_dates = make_dataset(merged_aapl, WINDOW_SIZE)\n",
    "\n",
    "def build_gru_model(Xp, Xt):\n",
    "    in_price = Input(shape=(Xp.shape[1], Xp.shape[2]))\n",
    "    x1 = GRU(64, return_sequences=True)(in_price)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    x1 = GRU(16)(x1)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "\n",
    "    in_topic = Input(shape=(Xt.shape[1], Xt.shape[2]))\n",
    "    x2 = TimeDistributed(Dense(64, activation='relu'))(in_topic)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = TimeDistributed(Dense(32, activation='relu'))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = GlobalAveragePooling1D()(x2)\n",
    "\n",
    "    x = concatenate([x1, x2])\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=[in_price, in_topic], outputs=out)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')\n",
    "    return model\n",
    "\n",
    "def walk_forward_gru_no_retrace(Xp, Xt, y, dates, start=0.8, epochs=15):\n",
    "    n_total = len(y)\n",
    "    n_train = int(n_total * start)\n",
    "    y_pred, y_true = [], []\n",
    "    start_all = time.time()\n",
    "    model = build_gru_model(Xp, Xt)\n",
    "    for i in tqdm(range(n_train, n_total), desc=\"Walk-Forward GRU (No Retrace)\"):\n",
    "        t0 = time.time()\n",
    "        model.fit([Xp[:i], Xt[:i]], y[:i], epochs=epochs, batch_size=32, verbose=0)\n",
    "        pred_fn = model.predict\n",
    "        pred = pred_fn([Xp[i:i+1], Xt[i:i+1]], verbose=0)[0][0]\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(y[i])\n",
    "        print(f\"Step {i}/{n_total} - Time: {time.time() - t0:.2f}s\")\n",
    "    print(f\"Total time: {(time.time() - start_all)/60:.2f} minutes\")\n",
    "    return y_true, y_pred\n",
    "\n",
    "def evaluate_baseline(y_true, y_pred, label=\"GRU\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\" {label}  MAE: {mae:.2f}, RMSE: {rmse:.2f}, R: {r2:.4f}\")\n",
    "    return mae, rmse, r2\n",
    "\n",
    "def baseline_random_walk(y_true):\n",
    "    return y_true[:-1], y_true[1:]\n",
    "\n",
    "def baseline_arima(df_close):\n",
    "    history = list(df_close[:WINDOW_SIZE])\n",
    "    predictions = []\n",
    "    for t in range(WINDOW_SIZE, len(df_close)-1):\n",
    "        model = ARIMA(history, order=(5,1,0))\n",
    "        model_fit = model.fit()\n",
    "        output = model_fit.forecast()\n",
    "        predictions.append(output[0])\n",
    "        history.append(df_close[t])\n",
    "    return df_close[WINDOW_SIZE+1:], predictions[1:]\n",
    "\n",
    "def baseline_sarima(df_close):\n",
    "    history = list(df_close[:WINDOW_SIZE])\n",
    "    predictions = []\n",
    "    for t in range(WINDOW_SIZE, len(df_close)-1):\n",
    "        model = SARIMAX(history, order=(1,1,1), seasonal_order=(1,1,1,12))\n",
    "        model_fit = model.fit(disp=False)\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(df_close[t])\n",
    "    return df_close[WINDOW_SIZE+1:], predictions[1:]\n",
    "\n",
    "print(\"\\n Training GRU with Walk-Forward Validation + ETA...\")\n",
    "y_true_gru, y_pred_gru = walk_forward_gru_no_retrace(Xp, Xt, y, y_dates)\n",
    "evaluate_baseline(y_true_gru, y_pred_gru, \"GRU\")\n",
    "\n",
    "print(\"\\n Evaluating Baseline Models...\")\n",
    "y_rw_true, y_rw_pred = baseline_random_walk(list(y))\n",
    "evaluate_baseline(y_rw_true, y_rw_pred, \"Random Walk\")\n",
    "\n",
    "y_arima_true, y_arima_pred = baseline_arima(list(merged_aapl['close'].values))\n",
    "evaluate_baseline(y_arima_true, y_arima_pred, \"ARIMA\")\n",
    "\n",
    "y_sarima_true, y_sarima_pred = baseline_sarima(list(merged_aapl['close'].values))\n",
    "evaluate_baseline(y_sarima_true, y_sarima_pred, \"SARIMA\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_true_gru[:100], label=\"Actual\", marker='o')\n",
    "plt.plot(y_pred_gru[:100], label=\"GRU Predicted\", marker='x')\n",
    "plt.title(\"AAPL - GRU Walk-Forward Prediction (First 100 Days)\")\n",
    "plt.legend(); plt.grid(); plt.show()\n",
    "\n",
    "np.save(\"y_pred_gru.npy\", np.array(y_pred_gru))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df886fb1",
   "metadata": {},
   "source": [
    "Versi Pakai Expanding Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a33df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU with expanding-window validation (step=5) + baselines + metrics + plot\n",
    "#KALAU MAU PAKAI GPU\n",
    "#import tensorflow as tf\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "#if gpus:\n",
    "#    print(\" GPU tersedia:\", gpus)\n",
    "#    try:\n",
    "#        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "#    except RuntimeError as e:\n",
    "#        print(e)\n",
    "#else:\n",
    "#    print(\" GPU tidak tersedia, menggunakan CPU\")\n",
    "\n",
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Dropout, concatenate, TimeDistributed, BatchNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ======= Data Load =======\n",
    "PRICE_FEATURES = ['open', 'high', 'low', 'close', 'volume']\n",
    "aapl = pd.read_csv(\"AAPL_2020_2025.csv\")\n",
    "apple_df = pd.read_csv(\"apple_with_sentiment.csv\")\n",
    "for df in [aapl, apple_df]:\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "\n",
    "# ======= Topic Sentiment Pipeline =======\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "def run_topic_pipeline(news_df):\n",
    "    embeddings = embedding_model.encode(news_df['content'].tolist(), show_progress_bar=True)\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        calculate_probabilities=True,\n",
    "        min_topic_size=15,\n",
    "        low_memory=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(news_df['content'], embeddings)\n",
    "    news_df['topic_prob'] = [np.array(p) for p in probs]\n",
    "    num_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
    "    daily_topic_sent = {}\n",
    "    for date, group in news_df.groupby('date'):\n",
    "        topic_sent = np.zeros(num_topics)\n",
    "        topic_weight = np.zeros(num_topics)\n",
    "        for _, row in group.iterrows():\n",
    "            for i in range(num_topics):\n",
    "                topic_sent[i] += row['sentiment_score'] * row['topic_prob'][i]\n",
    "                topic_weight[i] += row['topic_prob'][i]\n",
    "        avg_sent = [topic_sent[i]/topic_weight[i] if topic_weight[i]!=0 else 0 for i in range(num_topics)]\n",
    "        daily_topic_sent[date] = avg_sent\n",
    "    sent_df = pd.DataFrame.from_dict(daily_topic_sent, orient='index')\n",
    "    sent_df.index = pd.to_datetime(sent_df.index).tz_localize(None).normalize()\n",
    "    return sent_df\n",
    "\n",
    "topic_aapl = run_topic_pipeline(apple_df)\n",
    "\n",
    "# ======= Merge & Prepare Dataset =======\n",
    "def merge_price_topic(stock_df, topic_df):\n",
    "    df = stock_df.copy()\n",
    "    df = df[df['date'] >= '2021-01-01']\n",
    "    df.set_index('date', inplace=True)\n",
    "    return pd.merge(df, topic_df, left_index=True, right_index=True, how='inner').dropna()\n",
    "\n",
    "merged_aapl = merge_price_topic(aapl, topic_aapl)\n",
    "WINDOW_SIZE = 360\n",
    "\n",
    "def make_dataset(df, window_size):\n",
    "    X_price, X_topic, y, dates = [], [], [], []\n",
    "    scaler = StandardScaler()\n",
    "    scaled_prices = scaler.fit_transform(df[PRICE_FEATURES])\n",
    "    for i in range(window_size, len(df) - 1):\n",
    "        price = scaled_prices[i-window_size:i]\n",
    "        topic = df.iloc[i-window_size:i, len(PRICE_FEATURES):].to_numpy(dtype=np.float32)\n",
    "        target = df.iloc[i+1]['close']\n",
    "        date = df.index[i+1]\n",
    "        X_price.append(price)\n",
    "        X_topic.append(topic)\n",
    "        y.append(target)\n",
    "        dates.append(date)\n",
    "    return (\n",
    "        np.array(X_price, dtype=np.float32),\n",
    "        np.array(X_topic, dtype=np.float32),\n",
    "        np.array(y, dtype=np.float32),\n",
    "        dates\n",
    "    )\n",
    "\n",
    "Xp, Xt, y, y_dates = make_dataset(merged_aapl, WINDOW_SIZE)\n",
    "\n",
    "# ======= GRU Model =======\n",
    "def build_gru_model(Xp, Xt):\n",
    "    in_price = Input(shape=(Xp.shape[1], Xp.shape[2]))\n",
    "    x1 = GRU(64, return_sequences=True)(in_price)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    x1 = GRU(16)(x1)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "\n",
    "    in_topic = Input(shape=(Xt.shape[1], Xt.shape[2]))\n",
    "    x2 = TimeDistributed(Dense(64, activation='relu'))(in_topic)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = TimeDistributed(Dense(32, activation='relu'))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = GlobalAveragePooling1D()(x2)\n",
    "\n",
    "    x = concatenate([x1, x2])\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=[in_price, in_topic], outputs=out)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')\n",
    "    return model\n",
    "\n",
    "# ======= Expanding Step=5 =======\n",
    "def walk_forward_gru_expanding_step5(Xp, Xt, y, dates, start=0.8, epochs=10):\n",
    "    n_total = len(y)\n",
    "    n_train = int(n_total * start)\n",
    "    y_pred, y_true = [], []\n",
    "    model = build_gru_model(Xp, Xt)\n",
    "    step_durations = []\n",
    "\n",
    "    for i in tqdm(range(n_train, n_total, 5), desc=\"Expanding Step=5\"):\n",
    "        start_time = time.time()\n",
    "        model.fit([Xp[:i], Xt[:i]], y[:i], epochs=epochs, batch_size=32, verbose=0)\n",
    "        for j in range(i, min(i+5, n_total)):\n",
    "            pred = model.predict([Xp[j:j+1], Xt[j:j+1]], verbose=0)[0][0]\n",
    "            y_pred.append(pred)\n",
    "            y_true.append(y[j])\n",
    "        step_time = time.time() - start_time\n",
    "        eta = np.mean(step_durations) * ((n_total - i - 1) // 5) / 60 if step_durations else 0\n",
    "        step_durations.append(step_time)\n",
    "        print(f\"Step {i}-{min(i+5, n_total)-1} done in {step_time:.2f}s | ETA: {eta:.2f} min\")\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "# ======= Evaluation =======\n",
    "def evaluate_baseline(y_true, y_pred, label=\"Model\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\" {label}  MAE: {mae:.2f}, RMSE: {rmse:.2f}, R: {r2:.4f}\")\n",
    "    return mae, rmse, r2\n",
    "\n",
    "def baseline_random_walk(y_true):\n",
    "    return y_true[:-1], y_true[1:]\n",
    "\n",
    "def baseline_arima(df_close):\n",
    "    history = list(df_close[:WINDOW_SIZE])\n",
    "    predictions = []\n",
    "    for t in range(WINDOW_SIZE, len(df_close)-1):\n",
    "        model = ARIMA(history, order=(5,1,0))\n",
    "        model_fit = model.fit()\n",
    "        output = model_fit.forecast()\n",
    "        predictions.append(output[0])\n",
    "        history.append(df_close[t])\n",
    "    return df_close[WINDOW_SIZE+1:], predictions[1:]\n",
    "\n",
    "def baseline_sarima(df_close):\n",
    "    history = list(df_close[:WINDOW_SIZE])\n",
    "    predictions = []\n",
    "    for t in range(WINDOW_SIZE, len(df_close)-1):\n",
    "        model = SARIMAX(history, order=(1,1,1), seasonal_order=(1,1,1,12))\n",
    "        model_fit = model.fit(disp=False)\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(df_close[t])\n",
    "    return df_close[WINDOW_SIZE+1:], predictions[1:]\n",
    "\n",
    "# ======= Run All =======\n",
    "print(\"\\n Training GRU with Expanding Step=5...\")\n",
    "y_true_gru, y_pred_gru = walk_forward_gru_expanding_step5(Xp, Xt, y, y_dates)\n",
    "\n",
    "np.save(\"y_true_gru_step5.npy\", np.array(y_true_gru))\n",
    "np.save(\"y_pred_gru_step5.npy\", np.array(y_pred_gru))\n",
    "\n",
    "evaluate_baseline(y_true_gru, y_pred_gru, \"GRU (Step=5)\")\n",
    "\n",
    "print(\"\\n Evaluating Baseline Models...\")\n",
    "y_rw_true, y_rw_pred = baseline_random_walk(list(y))\n",
    "evaluate_baseline(y_rw_true, y_rw_pred, \"Random Walk\")\n",
    "\n",
    "y_arima_true, y_arima_pred = baseline_arima(list(merged_aapl['close'].values))\n",
    "evaluate_baseline(y_arima_true, y_arima_pred, \"ARIMA\")\n",
    "\n",
    "y_sarima_true, y_sarima_pred = baseline_sarima(list(merged_aapl['close'].values))\n",
    "evaluate_baseline(y_sarima_true, y_sarima_pred, \"SARIMA\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_true_gru[:100], label=\"Actual\", marker='o')\n",
    "plt.plot(y_pred_gru[:100], label=\"GRU Predicted\", marker='x')\n",
    "plt.title(\"AAPL - GRU Expanding Step=5 Prediction (First 100 Days)\")\n",
    "plt.legend(); plt.grid(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
